{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "env: CMAKE_ARGS=\"-DGGML_BLAS=ON -DGGML_BLAS_VENDOR=OpenBLAS\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.4; however, version 24.3.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\SAHILM~1\\AppData\\Local\\Programs\\MUEDIT~1\\Python\\pythonw.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (4.46.2)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (0.2.0)\n",
      "Requirement already satisfied: protobuf in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (5.28.3)\n",
      "Requirement already satisfied: accelerate in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (1.0.1)\n",
      "Requirement already satisfied: kagglehub in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (0.2.9)\n",
      "Requirement already satisfied: peft in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: pandas in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (2.0.3)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from transformers) (4.67.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from transformers) (3.8.0)\n",
      "Requirement already satisfied: requests in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from transformers) (1.24.4)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from transformers) (0.26.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.10.0 in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from accelerate) (2.4.1+cu124)\n",
      "Requirement already satisfied: psutil in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from accelerate) (6.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.10.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
      "Requirement already satisfied: sympy in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from torch>=1.10.0->accelerate) (1.13.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from requests->transformers) (1.26.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from requests->transformers) (2022.9.24)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Looking in indexes: https://pypi.org/simple, https://abetlen.github.io/llama-cpp-python/whl/cpu\n",
      "Collecting llama-cpp-python\n",
      "  Using cached llama_cpp_python-0.3.1.tar.gz (63.9 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Installing backend dependencies: started\n",
      "  Installing backend dependencies: finished with status 'done'\n",
      "    Preparing wheel metadata: started\n",
      "    Preparing wheel metadata: finished with status 'done'\n",
      "Collecting diskcache>=5.6.1\n",
      "  Using cached diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "Requirement already satisfied: numpy>=1.20.0 in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from llama-cpp-python) (1.24.4)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from llama-cpp-python) (4.12.2)\n",
      "Requirement already satisfied: jinja2>=2.11.3 in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from llama-cpp-python) (3.1.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from jinja2>=2.11.3->llama-cpp-python) (2.1.5)\n",
      "Building wheels for collected packages: llama-cpp-python\n",
      "  Building wheel for llama-cpp-python (PEP 517): started\n",
      "  Building wheel for llama-cpp-python (PEP 517): still running...\n",
      "  Building wheel for llama-cpp-python (PEP 517): finished with status 'done'\n",
      "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.3.1-cp38-cp38-win_amd64.whl size=3186312 sha256=373c16761f3ab2e9fdcc48ad1376ebe4f841d342c1faa9b6e793b170834ef76f\n",
      "  Stored in directory: c:\\users\\sahil malhotra\\appdata\\local\\pip\\cache\\wheels\\88\\2f\\a1\\46402a763dc9820696497cfff72455d8f69e95ae335be5f2d6\n",
      "Successfully built llama-cpp-python\n",
      "Installing collected packages: diskcache, llama-cpp-python\n",
      "Successfully installed diskcache-5.6.3 llama-cpp-python-0.3.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.4; however, version 24.3.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\SAHILM~1\\AppData\\Local\\Programs\\MUEDIT~1\\Python\\pythonw.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bitsandbytes in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (0.44.1)\n",
      "Requirement already satisfied: torch in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from bitsandbytes) (2.4.1+cu124)\n",
      "Requirement already satisfied: numpy in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from bitsandbytes) (1.24.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from torch->bitsandbytes) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from torch->bitsandbytes) (3.1)\n",
      "Requirement already satisfied: fsspec in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from torch->bitsandbytes) (2024.10.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from torch->bitsandbytes) (1.13.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from torch->bitsandbytes) (3.1.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from torch->bitsandbytes) (3.8.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from jinja2->torch->bitsandbytes) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from sympy->torch->bitsandbytes) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.4; however, version 24.3.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\SAHILM~1\\AppData\\Local\\Programs\\MUEDIT~1\\Python\\pythonw.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jupyter in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (1.1.1)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: notebook in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from jupyter) (7.2.2)\n",
      "Requirement already satisfied: ipykernel in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from jupyter) (6.29.5)\n",
      "Requirement already satisfied: ipywidgets in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from jupyter) (8.1.5)\n",
      "Requirement already satisfied: jupyter-console in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from jupyter) (6.6.3)\n",
      "Requirement already satisfied: jupyterlab in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from jupyter) (4.2.5)\n",
      "Requirement already satisfied: nbconvert in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from jupyter) (7.16.4)\n",
      "Requirement already satisfied: ipython>=7.23.1 in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from ipykernel->jupyter) (8.6.0)\n",
      "Requirement already satisfied: pyzmq>=24 in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from ipykernel->jupyter) (24.0.1)\n",
      "Requirement already satisfied: comm>=0.1.1 in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from ipykernel->jupyter) (0.2.2)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from ipykernel->jupyter) (1.8.7)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from ipykernel->jupyter) (8.6.3)\n",
      "Requirement already satisfied: psutil in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from ipykernel->jupyter) (6.1.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from ipykernel->jupyter) (24.1)\n",
      "Requirement already satisfied: traitlets>=5.4.0 in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from ipykernel->jupyter) (5.14.3)\n",
      "Requirement already satisfied: nest-asyncio in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from ipykernel->jupyter) (1.6.0)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from ipykernel->jupyter) (5.7.2)\n",
      "Requirement already satisfied: tornado>=6.1 in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from ipykernel->jupyter) (6.2)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from ipykernel->jupyter) (0.1.6)\n",
      "Requirement already satisfied: colorama in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from ipython>=7.23.1->ipykernel->jupyter) (0.4.6)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from ipython>=7.23.1->ipykernel->jupyter) (0.7.5)\n",
      "Requirement already satisfied: backcall in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from ipython>=7.23.1->ipykernel->jupyter) (0.2.0)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>3.0.1 in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from ipython>=7.23.1->ipykernel->jupyter) (3.0.32)\n",
      "Requirement already satisfied: stack-data in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from ipython>=7.23.1->ipykernel->jupyter) (0.6.1)\n",
      "Requirement already satisfied: decorator in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from ipython>=7.23.1->ipykernel->jupyter) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from ipython>=7.23.1->ipykernel->jupyter) (0.18.1)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from ipython>=7.23.1->ipykernel->jupyter) (2.13.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->jupyter) (0.8.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from jupyter-client>=6.1.12->ipykernel->jupyter) (2.8.2)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.3 in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from jupyter-client>=6.1.12->ipykernel->jupyter) (8.5.0)\n",
      "Requirement already satisfied: zipp>=3.20 in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from importlib-metadata>=4.8.3->jupyter-client>=6.1.12->ipykernel->jupyter) (3.20.2)\n",
      "Requirement already satisfied: pywin32>=300 in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->jupyter) (305)\n",
      "Requirement already satisfied: platformdirs>=2.5 in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->jupyter) (2.5.4)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from prompt-toolkit<3.1.0,>3.0.1->ipython>=7.23.1->ipykernel->jupyter) (0.2.5)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from python-dateutil>=2.8.2->jupyter-client>=6.1.12->ipykernel->jupyter) (1.16.0)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.12 in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from ipywidgets->jupyter) (3.0.13)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.12 in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from ipywidgets->jupyter) (4.0.13)\n",
      "Requirement already satisfied: jupyter-server<3,>=2.4.0 in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from jupyterlab->jupyter) (2.14.2)\n",
      "Requirement already satisfied: httpx>=0.25.0 in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from jupyterlab->jupyter) (0.27.2)\n",
      "Requirement already satisfied: setuptools>=40.1.0 in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from jupyterlab->jupyter) (58.1.0)\n",
      "Requirement already satisfied: notebook-shim>=0.2 in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from jupyterlab->jupyter) (0.2.4)\n",
      "Requirement already satisfied: jinja2>=3.0.3 in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from jupyterlab->jupyter) (3.1.4)\n",
      "Requirement already satisfied: tomli>=1.2.2 in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from jupyterlab->jupyter) (1.2.3)\n",
      "Requirement already satisfied: jupyter-lsp>=2.0.0 in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from jupyterlab->jupyter) (2.2.5)\n",
      "Requirement already satisfied: jupyterlab-server<3,>=2.27.1 in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from jupyterlab->jupyter) (2.27.3)\n",
      "Requirement already satisfied: async-lru>=1.0.0 in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from jupyterlab->jupyter) (2.0.4)\n",
      "Requirement already satisfied: importlib-resources>=1.4 in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from jupyterlab->jupyter) (6.4.5)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from async-lru>=1.0.0->jupyterlab->jupyter) (4.12.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from httpx>=0.25.0->jupyterlab->jupyter) (1.3.1)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from httpx>=0.25.0->jupyterlab->jupyter) (1.0.6)\n",
      "Requirement already satisfied: certifi in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from httpx>=0.25.0->jupyterlab->jupyter) (2022.9.24)\n",
      "Requirement already satisfied: idna in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from httpx>=0.25.0->jupyterlab->jupyter) (3.4)\n",
      "Requirement already satisfied: anyio in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from httpx>=0.25.0->jupyterlab->jupyter) (4.5.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from httpcore==1.*->httpx>=0.25.0->jupyterlab->jupyter) (0.14.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from jinja2>=3.0.3->jupyterlab->jupyter) (2.1.5)\n",
      "Requirement already satisfied: pywinpty>=2.0.1 in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (2.0.13)\n",
      "Requirement already satisfied: overrides>=5.0 in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (7.7.0)\n",
      "Requirement already satisfied: jupyter-events>=0.9.0 in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (0.10.0)\n",
      "Requirement already satisfied: argon2-cffi>=21.1 in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (23.1.0)\n",
      "Requirement already satisfied: nbformat>=5.3.0 in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (5.10.4)\n",
      "Requirement already satisfied: send2trash>=1.8.2 in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (1.8.3)\n",
      "Requirement already satisfied: terminado>=0.8.3 in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (0.18.1)\n",
      "Requirement already satisfied: websocket-client>=1.7 in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (1.8.0)\n",
      "Requirement already satisfied: prometheus-client>=0.9 in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (0.21.0)\n",
      "Requirement already satisfied: jupyter-server-terminals>=0.4.4 in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (0.5.3)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from anyio->httpx>=0.25.0->jupyterlab->jupyter) (1.2.2)\n",
      "Requirement already satisfied: argon2-cffi-bindings in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (21.2.0)\n",
      "Requirement already satisfied: rfc3986-validator>=0.1.1 in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (0.1.1)\n",
      "Requirement already satisfied: jsonschema[format-nongpl]>=4.18.0 in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (4.23.0)\n",
      "Requirement already satisfied: referencing in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (0.35.1)\n",
      "Requirement already satisfied: pyyaml>=5.3 in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (6.0.2)\n",
      "Requirement already satisfied: python-json-logger>=2.0.4 in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (2.0.7)\n",
      "Requirement already satisfied: rfc3339-validator in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (0.1.4)\n",
      "Requirement already satisfied: pkgutil-resolve-name>=1.3.10 in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (1.3.10)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (2023.12.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (0.20.1)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (24.2.0)\n",
      "Requirement already satisfied: isoduration in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (20.11.0)\n",
      "Requirement already satisfied: jsonpointer>1.13 in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (3.0.0)\n",
      "Requirement already satisfied: uri-template in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (1.3.0)\n",
      "Requirement already satisfied: webcolors>=24.6.0 in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (24.8.0)\n",
      "Requirement already satisfied: fqdn in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (1.5.1)\n",
      "Requirement already satisfied: requests>=2.31 in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (2.32.3)\n",
      "Requirement already satisfied: json5>=0.9.0 in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (0.9.25)\n",
      "Requirement already satisfied: babel>=2.10 in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (2.16.0)\n",
      "Requirement already satisfied: pytz>=2015.7 in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from babel>=2.10->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (2024.2)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from nbconvert->jupyter) (1.5.1)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from nbconvert->jupyter) (0.10.0)\n",
      "Requirement already satisfied: bleach!=5.0.0 in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from nbconvert->jupyter) (6.1.0)\n",
      "Requirement already satisfied: jupyterlab-pygments in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from nbconvert->jupyter) (0.3.0)\n",
      "Requirement already satisfied: tinycss2 in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from nbconvert->jupyter) (1.4.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from nbconvert->jupyter) (4.12.3)\n",
      "Requirement already satisfied: defusedxml in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from nbconvert->jupyter) (0.7.1)\n",
      "Requirement already satisfied: mistune<4,>=2.0.3 in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from nbconvert->jupyter) (3.0.2)\n",
      "Requirement already satisfied: webencodings in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from bleach!=5.0.0->nbconvert->jupyter) (0.5.1)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from nbformat>=5.3.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (2.20.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from requests>=2.31->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (1.26.12)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from requests>=2.31->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (2.1.1)\n",
      "Requirement already satisfied: cffi>=1.0.1 in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (1.17.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (2.22)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from beautifulsoup4->nbconvert->jupyter) (2.6)\n",
      "Requirement already satisfied: arrow>=0.15.0 in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (1.3.0)\n",
      "Requirement already satisfied: types-python-dateutil>=2.8.10 in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (2.9.0.20241003)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter) (2.1.0)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter) (0.2.2)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter) (1.2.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.4; however, version 24.3.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\SAHILM~1\\AppData\\Local\\Programs\\MUEDIT~1\\Python\\pythonw.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipywidgets in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (8.1.5)\n",
      "Requirement already satisfied: comm>=0.1.3 in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from ipywidgets) (8.6.0)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.12 in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from ipywidgets) (4.0.13)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.12 in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from ipywidgets) (3.0.13)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: backcall in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: decorator in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (2.13.0)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>3.0.1 in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.32)\n",
      "Requirement already satisfied: colorama in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.4.6)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: stack-data in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.6.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.18.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from prompt-toolkit<3.1.0,>3.0.1->ipython>=6.1.0->ipywidgets) (0.2.5)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (1.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.1.0)\n",
      "Requirement already satisfied: six in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from asttokens>=2.1.0->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.4; however, version 24.3.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\SAHILM~1\\AppData\\Local\\Programs\\MUEDIT~1\\Python\\pythonw.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu124\n",
      "Requirement already satisfied: torch in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (2.4.1+cu124)\n",
      "Requirement already satisfied: torchvision in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (0.19.1+cu124)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (2.4.1+cu124)\n",
      "Requirement already satisfied: filelock in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from torch) (3.8.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: networkx in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: fsspec in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from torchvision) (1.24.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from torchvision) (10.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\sahil malhotra\\appdata\\local\\programs\\mu editor\\python\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.4; however, version 24.3.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\SAHILM~1\\AppData\\Local\\Programs\\MUEDIT~1\\Python\\pythonw.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers sentencepiece protobuf accelerate kagglehub peft pandas\n",
    "# Windows\n",
    "%set_env CMAKE_ARGS = \"-DGGML_BLAS=ON -DGGML_BLAS_VENDOR=OpenBLAS\"\n",
    "%pip install llama-cpp-python  --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cpu\n",
    "%pip install -U bitsandbytes\n",
    "%pip install --upgrade jupyter\n",
    "%pip install --upgrade ipywidgets\n",
    "# get command for your os here - https://pytorch.org/get-started/locally/#windows-pip\n",
    "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Hey! I have been feeling a bit down lately. I have no motivation to get any work done or keep up with my responsibilities. My relationships and career are suffering because of this. Sometimes I feel like it would be easier if I just didn't exist, like it would all be better if I was dead. Its been like this for months now.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "MODEL_PATH = 'klyang/MentaLLaMA-chat-7B'\n",
    "tokenizer = LlamaTokenizer.from_pretrained(MODEL_PATH)\n",
    "model = LlamaForCausalLM.from_pretrained(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Generate\n",
    "generate_ids = model.generate(inputs.input_ids, max_length=100)\n",
    "response = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model was what we originally planned to use. It was popular enough with multiple downloads and part of a larger mental health project called  MentaLLaM that seemed promising. Within that project this one seemed to be the most popular and most reccomended to use. However upon running this model, it takes way too long to generate any text output even on a 3080ti. Execution was cancelled after waiting for 10 minutes for a response since this would be too slow for any practical purposes any how."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "tokenizer = BartTokenizer.from_pretrained('Tianlin668/MentalBART')\n",
    "model = BartForConditionalGeneration.from_pretrained('Tianlin668/MentalBART')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full output: depression. Reasoning: The post mentions feeling down, lacking motivation, and experiencing a decline in relationships and career. The individual also expresses thoughts of wanting to not exist and feeling like it would be better if they were dead. These are common symptoms associated with depression, such as low mood, loss of interest, and suicidal ideation.\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Generate\n",
    "generate_ids = model.generate(inputs.input_ids, max_length=2048)\n",
    "response = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "print('Full output: ' + response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is the smaller version of the original proposed model and works incredibly quickly, responding in just a few seconds. Unfortunately, this model is not what we were expecting as it is trained on providing diagnoses as opposed to responding as a therpist would. While this is incredibly cool and useful, its not exactly what we were looking for in terms of creating a mental health chat bot. This is likely going to be hard to fine-tune/train for our purpose. That said this does seem like a really useful diagnosis tool so perhaps there is potential to leverage this in our final solution somehow in tandem with another model that actually responds to tyhe patient as a therapist would. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2.\n401 Client Error. (Request ID: Root=1-672e2060-46e2b69170eac95c725ba4af;b4bd8719-6b7f-406f-9e72-78c9756caf2a)\n\nCannot access gated repo for url https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2/resolve/main/config.json.\nAccess to model mistralai/Mistral-7B-Instruct-v0.2 is restricted. You must have access to it and be authenticated to access it. Please log in.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mC:\\Users\\SAHILM~1\\AppData\\Local\\Programs\\MUEDIT~1\\Python\\lib\\site-packages\\huggingface_hub\\utils\\_http.py:406\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    405\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 406\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    407\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mC:\\Users\\SAHILM~1\\AppData\\Local\\Programs\\MUEDIT~1\\Python\\lib\\site-packages\\requests\\models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2/resolve/main/config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mGatedRepoError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32mC:\\Users\\SAHILM~1\\AppData\\Local\\Programs\\MUEDIT~1\\Python\\lib\\site-packages\\transformers\\utils\\hub.py:403\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    401\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m--> 403\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    416\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mC:\\Users\\SAHILM~1\\AppData\\Local\\Programs\\MUEDIT~1\\Python\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Users\\SAHILM~1\\AppData\\Local\\Programs\\MUEDIT~1\\Python\\lib\\site-packages\\huggingface_hub\\file_download.py:862\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[1;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[0;32m    861\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 862\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    863\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[0;32m    864\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[0;32m    866\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    867\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    868\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    870\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[0;32m    871\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    872\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    873\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[0;32m    877\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Users\\SAHILM~1\\AppData\\Local\\Programs\\MUEDIT~1\\Python\\lib\\site-packages\\huggingface_hub\\file_download.py:969\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[1;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[0;32m    968\u001b[0m     \u001b[38;5;66;03m# Otherwise, raise appropriate error\u001b[39;00m\n\u001b[1;32m--> 969\u001b[0m     \u001b[43m_raise_on_head_call_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead_call_error\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    971\u001b[0m \u001b[38;5;66;03m# From now on, etag, commit_hash, url and size are not None.\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Users\\SAHILM~1\\AppData\\Local\\Programs\\MUEDIT~1\\Python\\lib\\site-packages\\huggingface_hub\\file_download.py:1484\u001b[0m, in \u001b[0;36m_raise_on_head_call_error\u001b[1;34m(head_call_error, force_download, local_files_only)\u001b[0m\n\u001b[0;32m   1482\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, RepositoryNotFoundError) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, GatedRepoError):\n\u001b[0;32m   1483\u001b[0m     \u001b[38;5;66;03m# Repo not found or gated => let's raise the actual error\u001b[39;00m\n\u001b[1;32m-> 1484\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m head_call_error\n\u001b[0;32m   1485\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1486\u001b[0m     \u001b[38;5;66;03m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Users\\SAHILM~1\\AppData\\Local\\Programs\\MUEDIT~1\\Python\\lib\\site-packages\\huggingface_hub\\file_download.py:1376\u001b[0m, in \u001b[0;36m_get_metadata_or_catch_error\u001b[1;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[0;32m   1375\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1376\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1377\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\n\u001b[0;32m   1378\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1379\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n",
      "File \u001b[1;32mC:\\Users\\SAHILM~1\\AppData\\Local\\Programs\\MUEDIT~1\\Python\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Users\\SAHILM~1\\AppData\\Local\\Programs\\MUEDIT~1\\Python\\lib\\site-packages\\huggingface_hub\\file_download.py:1296\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[1;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[0;32m   1295\u001b[0m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[1;32m-> 1296\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1297\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHEAD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1298\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1299\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1300\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1301\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1302\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1303\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1304\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1305\u001b[0m hf_raise_for_status(r)\n",
      "File \u001b[1;32mC:\\Users\\SAHILM~1\\AppData\\Local\\Programs\\MUEDIT~1\\Python\\lib\\site-packages\\huggingface_hub\\file_download.py:277\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[1;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[0;32m    276\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[1;32m--> 277\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    278\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    279\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    280\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    281\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    282\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    284\u001b[0m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Users\\SAHILM~1\\AppData\\Local\\Programs\\MUEDIT~1\\Python\\lib\\site-packages\\huggingface_hub\\file_download.py:301\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[1;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[0;32m    300\u001b[0m response \u001b[38;5;241m=\u001b[39m get_session()\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m--> 301\u001b[0m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mC:\\Users\\SAHILM~1\\AppData\\Local\\Programs\\MUEDIT~1\\Python\\lib\\site-packages\\huggingface_hub\\utils\\_http.py:423\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    420\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    421\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Client Error.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot access gated repo for url \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    422\u001b[0m     )\n\u001b[1;32m--> 423\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _format(GatedRepoError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    425\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m error_message \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccess to this resource is disabled.\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;31mGatedRepoError\u001b[0m: 401 Client Error. (Request ID: Root=1-672e2060-46e2b69170eac95c725ba4af;b4bd8719-6b7f-406f-9e72-78c9756caf2a)\n\nCannot access gated repo for url https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2/resolve/main/config.json.\nAccess to model mistralai/Mistral-7B-Instruct-v0.2 is restricted. You must have access to it and be authenticated to access it. Please log in.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [17], line 18\u001b[0m\n\u001b[0;32m     14\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Load tokenizer\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_bos_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mleft\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[0;32m     23\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Create peft model using base_model and finetuned adapter\u001b[39;00m\n\u001b[0;32m     26\u001b[0m config \u001b[38;5;241m=\u001b[39m PeftConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(adapter)\n",
      "File \u001b[1;32mC:\\Users\\SAHILM~1\\AppData\\Local\\Programs\\MUEDIT~1\\Python\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:877\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    875\u001b[0m         config \u001b[38;5;241m=\u001b[39m AutoConfig\u001b[38;5;241m.\u001b[39mfor_model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig_dict)\n\u001b[0;32m    876\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 877\u001b[0m         config \u001b[38;5;241m=\u001b[39m \u001b[43mAutoConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m config_tokenizer_class \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mtokenizer_class\n\u001b[0;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config\u001b[38;5;241m.\u001b[39mauto_map:\n",
      "File \u001b[1;32mC:\\Users\\SAHILM~1\\AppData\\Local\\Programs\\MUEDIT~1\\Python\\lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py:1017\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m   1014\u001b[0m trust_remote_code \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrust_remote_code\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m   1015\u001b[0m code_revision \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode_revision\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m-> 1017\u001b[0m config_dict, unused_kwargs \u001b[38;5;241m=\u001b[39m \u001b[43mPretrainedConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1018\u001b[0m has_remote_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoConfig\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1019\u001b[0m has_local_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m CONFIG_MAPPING\n",
      "File \u001b[1;32mC:\\Users\\SAHILM~1\\AppData\\Local\\Programs\\MUEDIT~1\\Python\\lib\\site-packages\\transformers\\configuration_utils.py:574\u001b[0m, in \u001b[0;36mPretrainedConfig.get_config_dict\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    572\u001b[0m original_kwargs \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(kwargs)\n\u001b[0;32m    573\u001b[0m \u001b[38;5;66;03m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[1;32m--> 574\u001b[0m config_dict, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    575\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    576\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {}, kwargs\n",
      "File \u001b[1;32mC:\\Users\\SAHILM~1\\AppData\\Local\\Programs\\MUEDIT~1\\Python\\lib\\site-packages\\transformers\\configuration_utils.py:633\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    629\u001b[0m configuration_file \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_configuration_file\u001b[39m\u001b[38;5;124m\"\u001b[39m, CONFIG_NAME) \u001b[38;5;28;01mif\u001b[39;00m gguf_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m gguf_file\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    632\u001b[0m     \u001b[38;5;66;03m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[1;32m--> 633\u001b[0m     resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    634\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    635\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfiguration_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    636\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    637\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    638\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    639\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    640\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    641\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    642\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    643\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    644\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    645\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    646\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    647\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m resolved_config_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    648\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, kwargs\n",
      "File \u001b[1;32mC:\\Users\\SAHILM~1\\AppData\\Local\\Programs\\MUEDIT~1\\Python\\lib\\site-packages\\transformers\\utils\\hub.py:421\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    419\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m resolved_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _raise_exceptions_for_gated_repo:\n\u001b[0;32m    420\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m resolved_file\n\u001b[1;32m--> 421\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    422\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to access a gated repo.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMake sure to have access to it at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    423\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    424\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    425\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RepositoryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    426\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    427\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a local folder and is not a valid model identifier \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    428\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlisted on \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mIf this is a private repository, make sure to pass a token \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    429\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhaving permission to this repo either by logging in with `huggingface-cli login` or by passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    430\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`token=<your_token>`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    431\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mOSError\u001b[0m: You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2.\n401 Client Error. (Request ID: Root=1-672e2060-46e2b69170eac95c725ba4af;b4bd8719-6b7f-406f-9e72-78c9756caf2a)\n\nCannot access gated repo for url https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2/resolve/main/config.json.\nAccess to model mistralai/Mistral-7B-Instruct-v0.2 is restricted. You must have access to it and be authenticated to access it. Please log in."
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from peft import PeftConfig, PeftModel\n",
    "\n",
    "base_model = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "adapter = \"GRMenon/mental-health-mistral-7b-instructv0.2-finetuned-V2\"\n",
    "\n",
    "token = \"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\"  # Replace with your actual token\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt }\n",
    "]\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model,\n",
    "    add_bos_token=True,\n",
    "    trust_remote_code=True,\n",
    "    padding_side='left'\n",
    ")\n",
    "\n",
    "# Create peft model using base_model and finetuned adapter\n",
    "config = PeftConfig.from_pretrained(adapter)\n",
    "model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path,\n",
    " load_in_4bit=True,\n",
    " device_map='auto',\n",
    " torch_dtype='auto',\n",
    " token=token)\n",
    "model = PeftModel.from_pretrained(model, adapter)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "input_ids = tokenizer.apply_chat_template(conversation=messages,\n",
    "  tokenize=True,\n",
    "  add_generation_prompt=True,\n",
    "  return_tensors='pt').to(device)\n",
    "output_ids = model.generate(input_ids=input_ids, max_new_tokens=512, do_sample=True, pad_token_id=2)\n",
    "response = tokenizer.batch_decode(output_ids.detach().cpu().numpy(), skip_special_tokens = True)\n",
    "\n",
    "# Model response: \n",
    "print(response[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authentication issue even though model is open source? License agreement has been accepted on hugging face - access token has been used. This model is not yet evaluated, can come back to debug and explore this in a follow up. See below for comparable model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "d4f56e7c6def4d05b56029cc5889735f",
      "e7ee179bb4904d198025d9495a9b5522",
      "0cd011ff8b1f4a35ac629ebff1e6409a",
      "43e56df9bd334208a7247729fb13a29b",
      "39f3878f3e0746a3ab5134d1942d68c6",
      "ed21ee4712b24efba1ad43fbc37b3489",
      "e722a8ce2a9e4aafaaa1bffbc5e8194d",
      "aaf02b509b274a6f99a9a95922ad3316",
      "c662dbce851343a6939f036324c91c6a",
      "36161f121dd84794837704841eb03d3b",
      "75d7c2a10eed469985d0c699504b0567"
     ]
    },
    "executionInfo": {
     "elapsed": 1241,
     "status": "ok",
     "timestamp": 1731005529184,
     "user": {
      "displayName": "Sahil Malhotra",
      "userId": "08914037324470894376"
     },
     "user_tz": 300
    },
    "id": "s7zLXfXhh4sI",
    "outputId": "5dc5235d-7394-40ea-c13a-c531f569af97"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53d25b24f6d94b458c38cb9ae5b66ff8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_path = \"prabureddy/Mental-Health-FineTuned-Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype='auto'\n",
    ").eval()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "C:\\Users\\SAHILM~1\\AppData\\Local\\Programs\\MUEDIT~1\\Python\\lib\\site-packages\\transformers\\models\\mistral\\modeling_mistral.py:456: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry you have been feeling down and motivated only by the thought of not existing! First thing I'd suggest is getting the help you need through therapy with a mental health professional. In the meantime, here are some things you can try:\n",
      "\n",
      "1. Take care of yourself physically. Get enough sleep, eat well, and exercise regularly.\n",
      "2. Connect with others who are going through similar things. You are not alone!\n",
      "3. Practice self-care activities that bring\n"
     ]
    }
   ],
   "source": [
    "# Prompt content: \"hi\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt }\n",
    "]\n",
    "\n",
    "input_ids = tokenizer.apply_chat_template(conversation=messages, tokenize=True, add_generation_prompt=True, return_tensors='pt')\n",
    "output_ids = model.generate(input_ids.to('cuda'), max_new_tokens=100).to(device)\n",
    "response = tokenizer.decode(output_ids[0][input_ids.shape[1]:], skip_special_tokens=True)\n",
    "\n",
    "# Model response: \"Hello! How can I assist you today?\"\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Response is helpful and useful, though a bit slow. About a minute and a half run time to generate 100 tokens isn't terrible. Output is cut off mid way through though. Higher token sizes will take even more processing time. Running this model on a 3080ti. Running in the cloud on streamlit or other free resource would likely not have enough processing power for this to be in a useable state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de1cc93e13e84a63b19ff082b7fee2d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"thrishala/mental_health_chatbot\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"thrishala/mental_health_chatbot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full output: Hey! I have been feeling a bit down lately. I have no motivation to get any work done or keep up with my responsibilities. My relationships and career are suffering because of this. Sometimes I feel like it would be easier if I just didn't exist, like it would all be better if I was dead. Its been like this for months now. [/INST] I'm sorry to hear that you're feeling this way. It's important to know that you're not alone and that there are people who care about you. I would like to ask you a few questions to better understand your situation. Can you tell me more about your daily life and how this feeling of hopelessness and lack of motivation has been affecting you? Additionally, have you experienced any significant changes or stressors in your life recently? This\n",
      "Theraspist resonse:  [/INST] I'm sorry to hear that you're feeling this way. It's important to know that you're not alone and that there are people who care about you. I would like to ask you a few questions to better understand your situation. Can you tell me more about your daily life and how this feeling of hopelessness and lack of motivation has been affecting you? Additionally, have you experienced any significant changes or stressors in your life recently? This\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "generate_ids = model.generate(**inputs, max_new_tokens=100)\n",
    "response = tokenizer.decode(generate_ids[0], skip_special_tokens=True)\n",
    "print('Full output: ' + response)\n",
    "print('Theraspist resonse: ' + response.split(prompt)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output responded appropriately, though this model was even slower taking about 3 minutes to generate 100 tokens and was still cut off at the end. This was again run on a 3080ti and therefor running for free on streamlit or some other free resource is likely not going to be a viable solution. Additionally this model seems to be completing text with a delimineter of [/inst] for instruction to distinguish between the patient and therapist. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d127388f8d74eabb8c0b09cf0530fce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"fadodr/finetuned_mental_health_therapy_original\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"fadodr/finetuned_mental_health_therapy_original\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 199,
     "status": "ok",
     "timestamp": 1731008279598,
     "user": {
      "displayName": "Sahil Malhotra",
      "userId": "08914037324470894376"
     },
     "user_tz": 300
    },
    "id": "amR_7XJ9weVM"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SAHILM~1\\AppData\\Local\\Programs\\MUEDIT~1\\Python\\lib\\site-packages\\transformers\\generation\\utils.py:2097: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full output: Hey! I have been feeling a bit down lately. I have no motivation to get any work done or keep up with my responsibilities. My relationships and career are suffering because of this. Sometimes I feel like it would be easier if I just didn't exist, like it would all be better if I was dead. Its been like this for months now. I'm not sure what to do. I just want to feel happy again. I feel like I'm letting everyone down. I feel like I'm a burden. I don't know what to do. I feel like I'm being punished. I don't know why I feel this way. I just want to feel happy again.\n",
      "I am very sorry to hear that you are feeling so down. I understand that you are feeling like you are a burden. I am also sorry to hear that you\n",
      "Theraspist resonse:  I'm not sure what to do. I just want to feel happy again. I feel like I'm letting everyone down. I feel like I'm a burden. I don't know what to do. I feel like I'm being punished. I don't know why I feel this way. I just want to feel happy again.\n",
      "I am very sorry to hear that you are feeling so down. I understand that you are feeling like you are a burden. I am also sorry to hear that you\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "generate_ids = model.generate(**inputs, max_new_tokens=100)\n",
    "response = tokenizer.decode(generate_ids[0], skip_special_tokens=True)\n",
    "print('Full output: ' + response)\n",
    "print('Theraspist resonse: ' + response.split(prompt)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model responed pretty quickly, taking only a few seconds on a 3080ti. This has postential in terms of performance, however the base model appeears to be more on text completion as opposed to response. The generated text is respondiong as a patient would, continuing the user's original input before outputting as a theapist would, seemingly at random. Thedelimiter to seperate the atcual therapist response compared to the text completion of the patient is a new line but the fact that we are wasting tokens to complete the patient input ios not useful. This could potentially be trained/fine-tuned further accordingly and has potential, though a better base model would be preffered. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 22 key-value pairs and 435 tensors from C:\\Users\\Sahil Malhotra\\.cache\\huggingface\\hub\\models--victunes--TherapyBeagle-11B-v2-GGUF\\snapshots\\9b2e72d9a895a47d0ea55d7d268b926b7bb19262\\.\\TherapyBeagle-11B-v2-Q2_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = therapybeagle-11B-v2-e2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 48\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 10\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% for message in messages %}{% if me...\n",
      "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   97 tensors\n",
      "llama_model_loader: - type q2_K:  193 tensors\n",
      "llama_model_loader: - type q3_K:   96 tensors\n",
      "llama_model_loader: - type q4_K:   48 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 48\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 34B\n",
      "llm_load_print_meta: model ftype      = Q2_K - Medium\n",
      "llm_load_print_meta: model params     = 10.73 B\n",
      "llm_load_print_meta: model size       = 3.73 GiB (2.98 BPW) \n",
      "llm_load_print_meta: general.name     = therapybeagle-11B-v2-e2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOG token        = 2 '</s>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.20 MiB\n",
      "llm_load_tensors:        CPU buffer size =  3817.07 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =    96.00 MiB\n",
      "llama_new_context_with_model: KV self size  =   96.00 MiB, K (f16):   48.00 MiB, V (f16):   48.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    81.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1542\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'general.name': 'therapybeagle-11B-v2-e2', 'general.architecture': 'llama', 'llama.context_length': '32768', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '4096', 'llama.block_count': '48', 'llama.feed_forward_length': '14336', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '2', 'general.file_type': '10', 'llama.attention.head_count_kv': '8', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '10000.000000', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.add_bos_token': 'true', 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.chat_template': \"{% for message in messages %}{% if message['role'] == 'system' %}{% if message['content']%}{{'### System:\\n' + message['content']+'\\n\\n'}}{% endif %}{% elif message['role'] == 'user' %}{{'### User:\\n' + message['content']+'\\n\\n'}}{% elif message['role'] == 'assistant' %}{{'### Assistant:\\n'  + message['content']}}{% endif %}{% if loop.last and add_generation_prompt %}{{ '### Assistant:\\n' }}{% endif %}{% endfor %}\"}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {% for message in messages %}{% if message['role'] == 'system' %}{% if message['content']%}{{'### System:\n",
      "' + message['content']+'\n",
      "\n",
      "'}}{% endif %}{% elif message['role'] == 'user' %}{{'### User:\n",
      "' + message['content']+'\n",
      "\n",
      "'}}{% elif message['role'] == 'assistant' %}{{'### Assistant:\n",
      "'  + message['content']}}{% endif %}{% if loop.last and add_generation_prompt %}{{ '### Assistant:\n",
      "' }}{% endif %}{% endfor %}\n",
      "Using chat eos_token: </s>\n",
      "Using chat bos_token: <s>\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "llm = Llama.from_pretrained(\n",
    "\trepo_id=\"victunes/TherapyBeagle-11B-v2-GGUF\",\n",
    "\tfilename=\"TherapyBeagle-11B-v2-Q2_K.gguf\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 82 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2783.28 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    57 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    9998.92 ms /    58 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Hi Cavanaugh, thank you for reaching out. I'm really sorry to hear that you're feeling this way. It takes a lot of courage to share such thoughts. Can you tell me more about what's been going on in your life recently?\\n\\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = llm.create_chat_completion(\n",
    "\tmessages = [\n",
    "\t\t{\n",
    "\t\t\t\"role\": \"user\",\n",
    "\t\t\t\"content\": prompt\n",
    "\t\t}\n",
    "\t]\n",
    ")\n",
    "response['choices'][0]['message']['content']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model require cpp processing and that is not an environment we could reasonably set up on streamlit as installing and compiling the cpp dependencies alone took over half an hour. Regardless, the model was run for experimentatioin purposes in case it was useable. Testing showed a relatively quick model, taking only a few seconds to respond as a therapist would, however mostly the output was about asking for more information and not reeally addressing anything that the patient mentioned in the prompt. This can be good to continue the conversation but may not be as useful in terms of helping the patient. More experimentation can be done here and its possible this model can be trained/fine-tuned accoridngly to be more useful. The quick response time shows some promise, though the complex cpp environmental set up could pose difficult. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "below is code to download and extract the Kaggle Mental Health Conversations dataset for training and refinement of existing models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version, please consider updating (latest version: 0.3.3)\n",
      "Path to dataset files: C:\\Users\\Sahil Malhotra\\.cache\\kagglehub\\datasets\\thedevastator\\nlp-mental-health-conversations\\versions\\2\n",
      "File already exists at: c:\\Users\\Sahil Malhotra\\Downloads\\kaggle-therapy-conversations.csv\n",
      "\n",
      "Conversation 1:\n",
      "Patient: I'm going through some things with my feelings and myself. I barely sleep and I do nothing but think about how I'm worthless and how I shouldn't be here.\n",
      "   I've never tried or contemplated suicide. I've always wanted to fix my issues, but I never get around to it.\n",
      "   How can I change my feeling of being worthless to everyone?\n",
      "Therapist: If everyone thinks you're worthless, then maybe you need to find new people to hang out with.Seriously, the social context in which a person lives is a big influence in self-esteem.Otherwise, you can go round and round trying to understand why you're not worthless, then go back to the same crowd and be knocked down again.There are many inspirational messages you can find in social media.  Maybe read some of the ones which state that no person is worthless, and that everyone has a good purpose to their life.Also, since our culture is so saturated with the belief that if someone doesn't feel good about themselves that this is somehow terrible.Bad feelings are part of living.  They are the motivation to remove ourselves from situations and relationships which do us more harm than good.Bad feelings do feel terrible.   Your feeling of worthlessness may be good in the sense of motivating you to find out that you are much better than your feelings today.\n",
      "\n",
      "Conversation 2:\n",
      "Patient: I'm going through some things with my feelings and myself. I barely sleep and I do nothing but think about how I'm worthless and how I shouldn't be here.\n",
      "   I've never tried or contemplated suicide. I've always wanted to fix my issues, but I never get around to it.\n",
      "   How can I change my feeling of being worthless to everyone?\n",
      "Therapist: Hello, and thank you for your question and seeking advice on this. Feelings of worthlessness is unfortunately common. In fact, most people, if not all, have felt this to some degree at some point in their life. You are not alone. Changing our feelings is like changing our thoughts - it's hard to do. Our minds are so amazing that the minute you change your thought another one can be right there to take it's place. Without your permission, another thought can just pop in there. The new thought may feel worse than the last one! My guess is that you have tried several things to improve this on your own even before reaching out on here. People often try thinking positive thoughts, debating with their thoughts, or simply telling themselves that they need to \"snap out of it\" - which is also a thought that carries some self-criticism. Some people try a different approach, and there are counselors out there that can help you with this. The idea is that instead of trying to change the thoughts, you change how you respond to them. You learn skills that allow you to manage difficult thoughts and feelings differently so they don't have the same impact on you that they do right now. For some people, they actually DO begin to experience less hurtful thoughts once they learn how to manage the ones they have differently. Acceptance and Commitment Therapy may be a good choice for you. There is information online and even self-help books that you can use to teach you the skills that I mentioned. Because they are skills, they require practice, but many people have found great relief and an enriched life by learning them. As for suicidal thoughts, I am very glad to read that this has not happened to you. Still, you should watch out for this because it can be a sign of a worsening depression. If you begin to think about this, it is important to reach out to a support system right away. The National Suicide Prevention Lifeline is 1-800-273-8255. The text line is #741741. I hope some other colleagues will provide you more suggestions. Be well...Robin Landwehr, DBH, LPCC\n",
      "\n",
      "Conversation 3:\n",
      "Patient: I'm going through some things with my feelings and myself. I barely sleep and I do nothing but think about how I'm worthless and how I shouldn't be here.\n",
      "   I've never tried or contemplated suicide. I've always wanted to fix my issues, but I never get around to it.\n",
      "   How can I change my feeling of being worthless to everyone?\n",
      "Therapist: First thing I'd suggest is getting the sleep you need or it will impact how you think and feel. I'd look at finding what is going well in your life and what you can be grateful for. I believe everyone has talents and wants to find their purpose in life. I think you can figure it out with some help.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import kagglehub\n",
    "import shutil\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"thedevastator/nlp-mental-health-conversations\")\n",
    "source_filename = \"train.csv\"\n",
    "\n",
    "print(\"Path to dataset files:\", path)\n",
    "\n",
    "new_csv_name = \"kaggle-therapy-conversations.csv\"\n",
    "\n",
    "patient_col='Context'\n",
    "therapist_col='Response'\n",
    "\n",
    "# Construct the source file path\n",
    "source_path = os.path.join(path, source_filename)\n",
    "\n",
    "# Define the destination path in the current working directory\n",
    "destination_path = os.path.join(os.getcwd(), new_csv_name)\n",
    "\n",
    "if not os.path.exists(destination_path):\n",
    "    # Copy the downloaded file to the new_csv_path\n",
    "\n",
    "    shutil.copyfile(source_path, destination_path)\n",
    "    print(f\"File copied to: {destination_path}\")\n",
    "else:\n",
    "    print(f\"File already exists at: {destination_path}\")\n",
    "\n",
    "df = pd.read_csv(destination_path)\n",
    "\n",
    "if patient_col not in df.columns or therapist_col not in df.columns:\n",
    "  raise ValueError(f\"CSV must contain '{patient_col}' and '{therapist_col}' columns\")\n",
    "\n",
    "conversations = list(zip(df[patient_col], df[therapist_col]))\n",
    "\n",
    "# Remove any rows where either text is NaN\n",
    "conversations = [(p, t) for p, t in conversations if pd.notna(p) and pd.notna(t)]\n",
    "\n",
    "# Print first few conversations to verify\n",
    "for i, (patient, therapist) in enumerate(conversations[:3]):\n",
    "  print(f\"\\nConversation {i + 1}:\")\n",
    "  print(f\"Patient: {patient}\")\n",
    "  print(f\"Therapist: {therapist}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "below is code to download and extract the daic-woz dataset. This process preoved to be incredibly slow despit the zip files only being a few MB in size. This limitation is likely due to poor resources and performance on the host machine that we are downloading from as opposed to my own internet conenction. Each zip file takes about 5 minutes to download. Leaving this running over night, some of the files have downloaded (370/492). Preprocessing and use for training and refinement of any model can happen once all files are fully downloaded, at the rate the download is taking this will likely need to left running for a full 24 hours or so, which was not originally accoutned for. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 196,
     "status": "ok",
     "timestamp": 1731006892837,
     "user": {
      "displayName": "Sahil Malhotra",
      "userId": "08914037324470894376"
     },
     "user_tz": 300
    },
    "id": "eajhZQrvCKHw"
   },
   "outputs": [],
   "source": [
    "zip_files = [\n",
    "    \"300_P.zip\",\n",
    "    \"301_P.zip\",\n",
    "    \"302_P.zip\",\n",
    "    \"303_P.zip\",\n",
    "    \"304_P.zip\",\n",
    "    \"305_P.zip\",\n",
    "    \"306_P.zip\",\n",
    "    \"307_P.zip\",\n",
    "    \"308_P.zip\",\n",
    "    \"309_P.zip\",\n",
    "    \"310_P.zip\",\n",
    "    \"311_P.zip\",\n",
    "    \"312_P.zip\",\n",
    "    \"313_P.zip\",\n",
    "    \"314_P.zip\",\n",
    "    \"315_P.zip\",\n",
    "    \"316_P.zip\",\n",
    "    \"317_P.zip\",\n",
    "    \"318_P.zip\",\n",
    "    \"319_P.zip\",\n",
    "    \"320_P.zip\",\n",
    "    \"321_P.zip\",\n",
    "    \"322_P.zip\",\n",
    "    \"323_P.zip\",\n",
    "    \"324_P.zip\",\n",
    "    \"325_P.zip\",\n",
    "    \"326_P.zip\",\n",
    "    \"327_P.zip\",\n",
    "    \"328_P.zip\",\n",
    "    \"329_P.zip\",\n",
    "    \"330_P.zip\",\n",
    "    \"331_P.zip\",\n",
    "    \"332_P.zip\",\n",
    "    \"333_P.zip\",\n",
    "    \"334_P.zip\",\n",
    "    \"335_P.zip\",\n",
    "    \"336_P.zip\",\n",
    "    \"337_P.zip\",\n",
    "    \"338_P.zip\",\n",
    "    \"339_P.zip\",\n",
    "    \"340_P.zip\",\n",
    "    \"341_P.zip\",\n",
    "    \"343_P.zip\",\n",
    "    \"344_P.zip\",\n",
    "    \"345_P.zip\",\n",
    "    \"346_P.zip\",\n",
    "    \"347_P.zip\",\n",
    "    \"348_P.zip\",\n",
    "    \"349_P.zip\",\n",
    "    \"350_P.zip\",\n",
    "    \"351_P.zip\",\n",
    "    \"352_P.zip\",\n",
    "    \"353_P.zip\",\n",
    "    \"354_P.zip\",\n",
    "    \"355_P.zip\",\n",
    "    \"356_P.zip\",\n",
    "    \"357_P.zip\",\n",
    "    \"358_P.zip\",\n",
    "    \"359_P.zip\",\n",
    "    \"360_P.zip\",\n",
    "    \"361_P.zip\",\n",
    "    \"362_P.zip\",\n",
    "    \"363_P.zip\",\n",
    "    \"364_P.zip\",\n",
    "    \"365_P.zip\",\n",
    "    \"366_P.zip\",\n",
    "    \"367_P.zip\",\n",
    "    \"368_P.zip\",\n",
    "    \"369_P.zip\",\n",
    "    \"370_P.zip\",\n",
    "    \"371_P.zip\",\n",
    "    \"372_P.zip\",\n",
    "    \"373_P.zip\",\n",
    "    \"374_P.zip\",\n",
    "    \"375_P.zip\",\n",
    "    \"376_P.zip\",\n",
    "    \"377_P.zip\",\n",
    "    \"378_P.zip\",\n",
    "    \"379_P.zip\",\n",
    "    \"380_P.zip\",\n",
    "    \"381_P.zip\",\n",
    "    \"382_P.zip\",\n",
    "    \"383_P.zip\",\n",
    "    \"384_P.zip\",\n",
    "    \"385_P.zip\",\n",
    "    \"386_P.zip\",\n",
    "    \"387_P.zip\",\n",
    "    \"388_P.zip\",\n",
    "    \"389_P.zip\",\n",
    "    \"390_P.zip\",\n",
    "    \"391_P.zip\",\n",
    "    \"392_P.zip\",\n",
    "    \"393_P.zip\",\n",
    "    \"395_P.zip\",\n",
    "    \"396_P.zip\",\n",
    "    \"397_P.zip\",\n",
    "    \"399_P.zip\",\n",
    "    \"400_P.zip\",\n",
    "    \"401_P.zip\",\n",
    "    \"402_P.zip\",\n",
    "    \"403_P.zip\",\n",
    "    \"404_P.zip\",\n",
    "    \"405_P.zip\",\n",
    "    \"406_P.zip\",\n",
    "    \"407_P.zip\",\n",
    "    \"408_P.zip\",\n",
    "    \"409_P.zip\",\n",
    "    \"410_P.zip\",\n",
    "    \"411_P.zip\",\n",
    "    \"412_P.zip\",\n",
    "    \"413_P.zip\",\n",
    "    \"414_P.zip\",\n",
    "    \"415_P.zip\",\n",
    "    \"416_P.zip\",\n",
    "    \"417_P.zip\",\n",
    "    \"418_P.zip\",\n",
    "    \"419_P.zip\",\n",
    "    \"420_P.zip\",\n",
    "    \"421_P.zip\",\n",
    "    \"422_P.zip\",\n",
    "    \"423_P.zip\",\n",
    "    \"424_P.zip\",\n",
    "    \"425_P.zip\",\n",
    "    \"426_P.zip\",\n",
    "    \"427_P.zip\",\n",
    "    \"428_P.zip\",\n",
    "    \"429_P.zip\",\n",
    "    \"430_P.zip\",\n",
    "    \"431_P.zip\",\n",
    "    \"432_P.zip\",\n",
    "    \"433_P.zip\",\n",
    "    \"434_P.zip\",\n",
    "    \"435_P.zip\",\n",
    "    \"436_P.zip\",\n",
    "    \"437_P.zip\",\n",
    "    \"438_P.zip\",\n",
    "    \"439_P.zip\",\n",
    "    \"440_P.zip\",\n",
    "    \"441_P.zip\",\n",
    "    \"442_P.zip\",\n",
    "    \"443_P.zip\",\n",
    "    \"444_P.zip\",\n",
    "    \"445_P.zip\",\n",
    "    \"446_P.zip\",\n",
    "    \"447_P.zip\",\n",
    "    \"448_P.zip\",\n",
    "    \"449_P.zip\",\n",
    "    \"450_P.zip\",\n",
    "    \"451_P.zip\",\n",
    "    \"452_P.zip\",\n",
    "    \"453_P.zip\",\n",
    "    \"454_P.zip\",\n",
    "    \"455_P.zip\",\n",
    "    \"456_P.zip\",\n",
    "    \"457_P.zip\",\n",
    "    \"458_P.zip\",\n",
    "    \"459_P.zip\",\n",
    "    \"461_P.zip\",\n",
    "    \"462_P.zip\",\n",
    "    \"463_P.zip\",\n",
    "    \"464_P.zip\",\n",
    "    \"465_P.zip\",\n",
    "    \"466_P.zip\",\n",
    "    \"467_P.zip\",\n",
    "    \"468_P.zip\",\n",
    "    \"469_P.zip\",\n",
    "    \"470_P.zip\",\n",
    "    \"471_P.zip\",\n",
    "    \"472_P.zip\",\n",
    "    \"473_P.zip\",\n",
    "    \"474_P.zip\",\n",
    "    \"475_P.zip\",\n",
    "    \"476_P.zip\",\n",
    "    \"477_P.zip\",\n",
    "    \"478_P.zip\",\n",
    "    \"479_P.zip\",\n",
    "    \"480_P.zip\",\n",
    "    \"481_P.zip\",\n",
    "    \"482_P.zip\",\n",
    "    \"483_P.zip\",\n",
    "    \"484_P.zip\",\n",
    "    \"485_P.zip\",\n",
    "    \"486_P.zip\",\n",
    "    \"487_P.zip\",\n",
    "    \"488_P.zip\",\n",
    "    \"489_P.zip\",\n",
    "    \"490_P.zip\",\n",
    "    \"491_P.zip\",\n",
    "    \"492_P.zip\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 722
    },
    "executionInfo": {
     "elapsed": 870629,
     "status": "error",
     "timestamp": 1731008263549,
     "user": {
      "displayName": "Sahil Malhotra",
      "userId": "08914037324470894376"
     },
     "user_tz": 300
    },
    "id": "UKBZW0_WDTDx",
    "outputId": "bb87ed8a-915c-4960-d930-ab2576261a3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: c:\\Users\\Sahil Malhotra\\Downloads\n",
      "Download directory: c:\\Users\\Sahil Malhotra\\Downloads\\daic-woz-file-zips\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'zip_files' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [9], line 26\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(extract_dir):\n\u001b[0;32m     24\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(extract_dir)\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_name \u001b[38;5;129;01min\u001b[39;00m \u001b[43mzip_files\u001b[49m:\n\u001b[0;32m     27\u001b[0m     download_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(download_dir, file_name)\n\u001b[0;32m     28\u001b[0m     extract_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(extract_dir, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msplitext(file_name)[\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'zip_files' is not defined"
     ]
    }
   ],
   "source": [
    "# Daic-Woz download\n",
    "# Downloading all of these is incredibly slow. It only did like 10 after an hour \n",
    "\n",
    "import os\n",
    "import zipfile\n",
    "import requests\n",
    "\n",
    "# Base URL of your website\n",
    "base_url = 'https://dcapswoz.ict.usc.edu/wwwdaicwoz/'\n",
    "\n",
    "# Create a directory to save the files\n",
    "download_dir = 'daic-woz-file-zips'\n",
    "extract_dir = 'daic-woz-files'\n",
    "\n",
    "# Print current working directory\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "\n",
    "# Print full absolute path of download directory\n",
    "print(\"Download directory:\", os.path.abspath(download_dir))\n",
    "\n",
    "if not os.path.exists(download_dir):\n",
    "    os.makedirs(download_dir)\n",
    "if not os.path.exists(extract_dir):\n",
    "    os.makedirs(extract_dir)\n",
    "\n",
    "for file_name in zip_files:\n",
    "    download_path = os.path.join(download_dir, file_name)\n",
    "    extract_path = os.path.join(extract_dir, os.path.splitext(file_name)[0])\n",
    "    \n",
    "    # Check if both zip file and extracted directory already exist\n",
    "    if os.path.exists(download_path) and os.path.exists(extract_path):\n",
    "        print(f\"Skipping {file_name} - already downloaded and extracted\")\n",
    "        continue\n",
    "    \n",
    "    # Download only if zip file doesn't exist\n",
    "    if not os.path.exists(download_path):\n",
    "        url = f\"{base_url}{file_name}\"\n",
    "        print(f\"Downloading: {file_name}\")\n",
    "        response = requests.get(url)\n",
    "    \n",
    "        with open(download_path, 'wb') as file:\n",
    "            file.write(response.content)\n",
    "            print(f\"Downloaded: {file_name}\")\n",
    "            \n",
    "    # Extract only if directory doesn't exist\n",
    "    if not os.path.exists(extract_path):\n",
    "        print(f\"Extracting: {file_name}\")\n",
    "        with zipfile.ZipFile(download_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(extract_path)\n",
    "            print(f\"Extracted: {file_name}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOcUg4MF2h3Wj4vlU+fBeZ0",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "\"Python/Mu (mu_venv-38-20240515-011952)\"",
   "language": "python",
   "name": "mu_venv-38-20240515-011952"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0cd011ff8b1f4a35ac629ebff1e6409a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_aaf02b509b274a6f99a9a95922ad3316",
      "max": 292,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c662dbce851343a6939f036324c91c6a",
      "value": 292
     }
    },
    "36161f121dd84794837704841eb03d3b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "39f3878f3e0746a3ab5134d1942d68c6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "43e56df9bd334208a7247729fb13a29b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_36161f121dd84794837704841eb03d3b",
      "placeholder": "​",
      "style": "IPY_MODEL_75d7c2a10eed469985d0c699504b0567",
      "value": " 292/292 [00:00&lt;00:00, 4.76kB/s]"
     }
    },
    "75d7c2a10eed469985d0c699504b0567": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "aaf02b509b274a6f99a9a95922ad3316": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c662dbce851343a6939f036324c91c6a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d4f56e7c6def4d05b56029cc5889735f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e7ee179bb4904d198025d9495a9b5522",
       "IPY_MODEL_0cd011ff8b1f4a35ac629ebff1e6409a",
       "IPY_MODEL_43e56df9bd334208a7247729fb13a29b"
      ],
      "layout": "IPY_MODEL_39f3878f3e0746a3ab5134d1942d68c6"
     }
    },
    "e722a8ce2a9e4aafaaa1bffbc5e8194d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e7ee179bb4904d198025d9495a9b5522": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ed21ee4712b24efba1ad43fbc37b3489",
      "placeholder": "​",
      "style": "IPY_MODEL_e722a8ce2a9e4aafaaa1bffbc5e8194d",
      "value": "generation_config.json: 100%"
     }
    },
    "ed21ee4712b24efba1ad43fbc37b3489": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
